# Data-Driven Procurement Red Flags: Prioritization and Implementation Plan

## Introduction

Public procurement represents a significant portion of government spending (around 12% of GDP in OECD countries\[1]) and is highly vulnerable to fraud and corruption. To safeguard integrity, analysts and watchdogs use **“red flag” indicators** – quantitative signals in procurement data that may hint at irregular or corrupt practices. These indicators are derived from patterns identified in academic research and international best practices.

For example, the Open Contracting Partnership (OCP) has compiled **73 risk indicators** spanning all stages of the procurement process (from planning to contract implementation) based on global literature and real cases\[2]. Multiple studies have shown that such red flags correlate with actual corruption; one analysis found that implementing a set of red-flag indicators could help identify known corrupt contracts in most cases\[3]\[4].

It is important to note that **one red flag alone is not proof of wrongdoing** – rather, each flag raises a hypothesis of risk that needs further investigation\[5]. In practice, a combination of flags or repeated occurrences strengthens the case for potential misconduct\[5]. The goal is to move from a **signal** to a **verified finding**: a flagged anomaly should prompt an inquiry, gathering evidence to confirm or refute the corruption hypothesis, ultimately leading to an audit report or enforcement action if warranted.

> **Figure:** Illustrative workflow of how a red flag signal can lead to a hypothesis of corruption, which is then investigated to produce proof and a report. A flag by itself only triggers a closer look—it takes investigation to confirm a problem.

---

## Prioritization of Indicators: Real-World vs. Conceptual

When selecting which integrity indicators to include, we will prioritize those that can be implemented end-to-end with available public data. The minimum viable product (MVP) will focus on data-driven flags that are directly computable using standardized datasets – in this case, U.S. federal procurement data from USAspending/FPDS and, by extension, any data published in the Open Contracting Data Standard (OCDS) format.

These are “real-world” indicators ready for automated detection and visualization in a prototype. For example, if we have fields for the number of bids, contract values, dates, etc., we can calculate indicators like single-bid awards or suspicious contract timing immediately. This pragmatic focus ensures an end-to-end demo: from raw data to computed flags to insights. It aligns with OCP’s approach of mapping red flag definitions to the data fields that are actually present in open datasets\[6]. Indeed, OCP’s open-source tool Cardinal was created to automate the calculation of common procurement risk indicators using OCDS data\[7], highlighting the importance of linking theory to available information.

At the same time, we acknowledge there are additional red flags suggested by academia and practitioners that may not be measurable with the current data alone. We will include a second tier of “conceptual” or advanced indicators as nice-to-have flags for the future. These will be clearly labeled as requiring additional data or integration with other systems (e.g. vendor-specific or bidder-level details that USAspending does not provide). For instance, detecting collusive bidding rings might require data on individual losing bids or corporate ownership links – information not included in standard contract award datasets. OCP notes that to calculate certain collusion risk indicators you need bid-specific data (such as all bid prices), which often isn’t published in basic procurement feeds\[8].

By separating these more complex flags, we avoid over-promising in the MVP while charting a path for future enhancements (for example, connecting to procurement portals or beneficial ownership registries to gather richer data). This two-tier prioritization ensures we deliver actionable analytics now, without losing sight of broader anti-corruption signals that could be incorporated later as data availability improves.

---

## Scope: Global Definitions vs. U.S. Implementation

Each risk indicator will be framed in a globally meaningful way but then tied to a specific U.S. data approximation for implementation. In practice, this means we first define the intent of the indicator using language and logic from international frameworks (such as OECD, World Bank, or Open Contracting guides) – ensuring that the concept is broadly applicable across jurisdictions. Next, we map that concept to concrete fields in USAspending (FPDS) to show how it can be computed in the U.S. context. This mapping makes clear which red flags we can directly calculate with U.S. federal procurement data (and how), versus which are conceptually important but not measurable with the MVP dataset. It also helps us remain consistent with global anti-corruption metrics while being transparent about data limitations.

For example, a well-known corruption risk indicator globally is “single-bid awards” – i.e. a supposedly competitive tender where only one vendor ends up bidding, often a sign of restricted competition or favoritism\[9]\[10]. The globally relevant definition might cite that single bidding undermines competition and is associated with higher corruption risk (as fewer bidders make collusion or rigging easier). To implement this in the U.S. context, we note that FPDS data has a field for “Number of Offers Received”, which captures how many bids were submitted for each contract\[11]. Thus, the USAspending approximation would flag any contract where `Number of Offers Received = 1` as a single-bid award.

We will perform this kind of mapping for each chosen indicator – e.g. if the global concept is “no public call for tender was published,” the U.S. proxy might be checking if the award was made under a non-competitive authority or an exception to open advertising. By explicitly documenting **(a)** the general definition and **(b)** the specific data fields or logic used in USAspending, we ensure clarity about what the flag is capturing in the MVP. This approach was inspired by Open Contracting’s methodology of linking each red flag to OCDS-standardized fields\[12]\[2], which enables replication in any country that publishes OCDS data. The result is that our indicators remain globally relevant (e.g. aligning with EU or World Bank integrity indicators) while being grounded in the U.S. reality for demonstration. The U.S. focus is strategic for the MVP because the federal dataset is comprehensive and accessible, allowing us to showcase the power of these analytics. However, the indicator definitions will be applicable internationally, and the solution can later be ported to other open contracting datasets with minimal adjustment.

In the following list, each red flag indicator is described in global terms and then mapped to USAspending data fields or approximations. Implementable (MVP) indicators are marked with **\[Data Flag]**, whereas more conceptual ones are marked with **\[Future Flag]**.

---

## Key Procurement Risk Indicators and Their Data Mappings

### 1. Single-Bid Competitions – **\[Data Flag]**
*   **Global Definition:** An ostensibly open tender in which only one bidder submits an offer. A low number of bidders (especially a single bid) can indicate restricted competition or collusion, as it suggests other potential suppliers were deterred or excluded\[9]. Single-bid awards have been widely used as a red flag in corruption risk models, reflecting reduced competition and higher corruption likelihood\[10]\[13]. For instance, the EU Single Market Scoreboard sets a benchmark that having over 20% of contracts with a single bidder is a “red” (high-risk) situation for a country\[14]\[15]. More competition is generally better, as it leads to better value for money and lowers corruption risks\[13].
*   **USAspending Implementation:** We can flag any contract award record where the “Number of Offers Received” is 1. FPDS (the source of USAspending’s contract data) explicitly records the count of bids received for each solicitation\[11]. Thus, if `Number_of_Offers_Received = 1`, we mark that procurement as a single-bid competition. In U.S. federal data, we may further refine this by excluding cases of legitimately non-competed awards (to separate from the “sole source” flag below) – e.g. the EU indicator counts single bids only for competitively tendered contracts\[16]. This flag is straightforward to compute and can be highlighted per agency or program. A high frequency of single-bid awards in an agency could warrant deeper analysis of whether requirements are being tailored or advertised inadequately. (Note: A related metric is the “single bidder rate” – the percentage of an agency’s contracts that had only one bid – which can be tracked over time as a performance indicator\[14].)

### 2. Non-Competitive or Limited Tender – **\[Data Flag]**
*   **Global Definition:** A contract awarded without an open call for bids, or using a restrictive procedure that limits competition. This includes direct awards (sole-source contracts) and negotiated tenders without public advertisement. Such scenarios bypass open competition and are universally recognized as a corruption risk if misused\[10]. For example, not publishing a call for tender or using an unjustified sole-source method can indicate favoritism or emergency procurement abuse\[10]. Fazekas & Kocsis identify the “no call for tender” and “non-open procedure” as key red flags in their cross-country corruption risk index\[10], since these practices can “unjustifiably restrict access to public contracts to favor a selected bidder”\[17].
*   **USAspending Implementation:** We will leverage FPDS fields like “Type of Competition” (a field indicating if the award was competed full and open, limited, or not competed) and “Solicitation Procedures”. A simple rule is to flag any award where “Extent of Competition” is coded as Not Competed or Not Available for Competition. Additionally, FPDS has a flag if a solicitation was publicly advertised on the government-wide point of entry (beta.SAM.gov) – if not, that could be a “no call for bids” situation. Thus, contracts awarded via sole-source justifications (e.g. only one source, urgency, expert services exemptions) will trigger this flag. We will document the specific codes (for example, FPDS codes like “ONLY ONE SOURCE” or “URGENT” under Reason Not Competed). High rates of non-competitive awards, especially for high-dollar contracts or without strong justification, are a serious red flag. This indicator is computable with USAspending data, though ensuring data quality (correct coding of competition) is important. We might exclude certain legitimate cases (e.g. small dollar micropurchases) or highlight them separately. Overall, this flag surfaces where lack of open competition could enable corruption.

### 3. Contract Value Splitting Around Thresholds – **\[Data Flag]**
*   **Global Definition:** Contract splitting refers to artificially breaking a procurement into multiple smaller contracts to evade thresholds that mandate competitive bidding or higher oversight. For example, if contracts just below a value threshold (like $250k) are repeatedly awarded, officials might be avoiding open tender requirements. This is a known tactic to bypass procurement rules, and researchers have found evidence of value manipulation around thresholds in real data\[18]. For instance, a study in Italy showed unusual clustering of contract values just under the limit for stricter tender procedures, indicating intentional parceling of contracts\[18]. As a red flag, one looks for multiple contracts of just under the same threshold awarded by the same agency or to the same vendor within a short timeframe.
*   **USAspending Implementation:** Using the “Action Obligation” (award amount) field, we can check for patterns where numerous awards fall just below key federal thresholds. U.S. procurement has thresholds for simplified acquisitions, sealed bidding, etc. – e.g., $250,000 is a common full-and-open competition threshold, and $7 million for certain set-asides, etc. We will flag clusters of contracts priced just under these thresholds (for example, $248K, $249K awards in suspicious frequency). A specific rule could be: if an agency issues multiple awards in the $240K-$250K range in a short period (say one fiscal quarter) rather than one combined contract, flag for potential splitting. Similarly, if one vendor receives a series of contracts each slightly under a threshold from the same buyer, that’s suspicious. This requires grouping the data by agency and time period to detect the pattern. It’s implementable with data, though defining the precise tolerance (what counts as “just below” – perhaps within 10% of the threshold) will be done in line with regulatory thresholds. This flag helps catch deliberate avoidance of competitive procedures by slicing procurements into pieces\[19]\[20].

### 4. High Vendor Concentration (Unusually Dominant Supplier) – **\[Data Flag]**
*   **Global Definition:** A situation where a single supplier wins a disproportionate share of contracts from a particular government buyer or within a category. While it’s possible for one supplier to legitimately be very successful, extreme concentration (especially if not explained by that supplier’s unique qualifications) can indicate collusion or favoritism. For example, if one company receives 50%+ of the contract value from a department over several years, it may suggest improper steering of contracts, kickback relationships, or suppression of competition. International guidance often recommends monitoring supplier diversity; a lack thereof (monopoly supplier) is a red flag for integrity. This can overlap with single-bid flags (if the same vendor keeps being the only bidder) or with patterns of contract awards always going to the same insider.
*   **USAspending Implementation:** We will compute the share of total procurement spend per vendor for each agency (or within each product/service category). If a vendor’s share exceeds a certain threshold (e.g. over 30% of an agency’s total contract value in a year), we flag that agency-vendor relationship for high concentration. We can present this visually as well – for instance, a chart of Agency X’s spending where Vendor A’s portion spikes dramatically over time could be revealing. Trends matter: if Vendor A had 5% last year but 50% this year, that sudden jump warrants a closer look (did procurement rules change, or did Vendor A perhaps gain undue influence?). Below is an example of how a dominant vendor pattern might appear:

    > **Example pattern:** A single contractor’s share of Agency X’s contract spending rises sharply over a short period, reaching over half of the agency’s total procurement value. Such a trend can signal possible favoritism or bid rigging.

    Using USAspending, this is implementable by grouping awards by Vendor (using the unique entity identifier) and summing values. We will account for the context (some agencies legitimately rely on one supplier for specialized needs, but generally a healthy market has multiple providers). As a benchmark, the EU scoreboard considered it concerning if the top 1% of suppliers get an overly large fraction of awards – our threshold can draw from such benchmarks or be set based on distribution analysis. This flag, combined with others (like if that same vendor’s contracts often have only one bidder or many non-competed awards), would greatly strengthen the hypothesis of collusion or corruption.

### 5. Excessive Contract Modifications – **\[Data Flag]**
*   **Global Definition:** Frequent or significant changes to a contract after award – for example, numerous amendments increasing the price or extending the duration. While some modifications are normal (due to changing requirements or unforeseen issues), an excessive number or value of modifications can indicate that the initial tender was under-scoped or under-priced deliberately to avoid scrutiny, with the contract later “padded” through change orders. It can also signal poor planning or contractor manipulation. Anti-corruption experts often cite unjustified amendments as a red flag\[21]. In particular, large price increases or time extensions, especially soon after contract signature, are suspicious. They may suggest that the true scope was hidden initially or that the contractor is renegotiating advantages post-award (possibly sharing benefits with corrupt officials).
*   **USAspending Implementation:** Federal contract data records modifications as separate entries linked to an initial award (with the same Contract ID but a modification number). We will aggregate contract award records to examine how many modifications each base contract has, and the cumulative change in value. A rule of thumb might be: flag any contract that has X or more modifications or where the total price increase via modifications exceeds Y% of the original award value. For example, if a contract originally awarded at $1 million ends up with modifications raising it to $3 million (a +200% increase), that’s a red flag. Or if a contract has, say, 10 modifications in a year, constantly changing scope, it may indicate opportunistic changes. We will use fields like “Base and Exercised Value” vs “Current Total Value” to compute percentage changes, and count modification entries. This flag is computable, though we will ensure to filter out administrative mods (e.g., address changes) versus those affecting money or duration. The OECD has noted that “excessive modifications during execution” were observed warning signs of corruption in procurement\[21], so our data-driven approach will catch those patterns in the US data. Essentially, this helps identify contracts that balloon in scope or cost beyond what was originally competitively bid – a situation ripe for abuse.

### 6. Price Outliers and Overruns – **\[Data Flag]** (partial)
*   **Global Definition:** Cases where the contract value is abnormally high compared to benchmarks – either compared to the pre-tender cost estimate or to similar purchases. Overpriced contracts can imply corruption (e.g., padding for kickbacks). Another angle is cost overruns during implementation: if final costs greatly exceed initial estimates or budgets, it could be due to corruption or inefficiency. In academic studies, one approach to detect corruption is to look at “unexplained high prices” or gaps between expected and actual costs\[22]\[23]. However, price flags often require baseline comparisons (market prices or engineer’s estimates).
*   **USAspending Implementation:** The feasibility of this flag is somewhat limited by data availability. USAspending does not directly provide an official pre-bid estimate for each contract. We do have the final award amount and can infer if modifications increased it (covered by the prior flag). For certain contracts (especially in construction), agencies might publish government cost estimates or independent government estimates, but those aren’t systematically in the data. As a proxy, we might flag contracts that far exceed the average price paid by other agencies for the same product/service. Using product/service codes or NAICS categories, we could compute average unit prices or cost per unit (if quantity info is present) and flag outliers. For example, if most agencies pay $100 each for a certain item but one contract pays $500 each, that’s an outlier to investigate. Another proxy: compare the winning bid to the next-best bid (but we typically lack losing bid info in USAspending – this would require separate data, so it edges into \[Future Flag] territory). For the MVP, we’ll note this as a data-driven flag we can partially implement by leveraging whatever comparative data is available (e.g., FPDS often has unit price and quantity for goods). We will label as a red flag any award where the price per unit or per similar service is, say, 2 standard deviations above the mean price paid by others. This requires more analysis and possibly external price references, so it will be a lower priority data flag. In summary, we can detect potential overpricing using cross-sectional comparisons in the data, but a full assessment of price fairness might need supplemental data (hence this sits between MVP and future enhancement).

### 7. Beneficial Ownership and Conflicts – **\[Future Flag]**
*   **Global Definition:** Flags related to the identities and relationships of companies and officials involved. For example, if two supposedly competing bidders in many tenders turn out to have the same owners or address, that’s a collusion red flag. If a winning supplier is linked to a politically exposed person or a family member of the procuring official, that’s a conflict-of-interest red flag. These situations are often not evident just from the procurement records but are critical red flags when data is available. International efforts like beneficial ownership transparency aim to enable checks for such links.
*   **USAspending Implementation:** Not directly implementable with current data. USAspending provides vendor names and identifiers (DUNS, UEI), but not information about corporate ownership or company principals. In the future, if we integrate a beneficial ownership database or a company registry (for example, linking the vendor’s identifier to OpenCorporates or a beneficial ownership register), we could flag cases such as: different vendors that share an address, phone, or owner (potential bid-rigging cartel); winning vendor’s executives have the same surname or address as an agency official (possible nepotism); vendor is registered in a known tax haven jurisdiction\[10]\[24] (could indicate an offshore shell company used to hide true owners). These are powerful indicators – for instance, researchers included a “tax haven supplier” flag in cross-national corruption indices\[10]\[24], reasoning that firms operating from secrecy jurisdictions raise integrity concerns. However, implementing these requires data beyond the standard contract award info. In our report, we will list these as conceptual flags and possibly demonstrate how they would be identified if such data were available (e.g. “Flag if Vendor country is in Financial Secrecy Index top 10”). This category remains a **future enhancement** – something to pursue by connecting additional datasets or through investigative analysis rather than automated detection in the MVP.

### 8. Collusion Indicators in Bidding Patterns – **\[Future Flag]**
*   **Global Definition:** Red flags that multiple bidders are colluding (bid-rigging cartels). Signs include suspicious bidding patterns like identical bid prices submitted by different vendors, bids that are too close to each other, rotating winners (companies take turns winning in a predictable order), or cover bidding (some bidders consistently submit uncompetitive high bids just to pretend there was competition). These require examining the set of bids per tender and sometimes behaviors across tenders. If, say, the same three companies bid on a series of projects and always one wins with the other two always losing in turn, it could be a cartel. Another flag is if losing bidders frequently subcontract to the winner afterwards – indicating they were colluding.
*   **USAspending Implementation:** Not possible with just USAspending data, since we only see the winning award. We do not see losing bids or detailed bid submissions. A true collusion detection would need either bid-level data from procurement systems or advanced analytics linking contract outcomes. Some countries publishing OCDS include bid details (like all bid prices, bidder names), enabling flags like “unusually close bid values” or detecting if the same IP address was used for submissions (if IT system data is available). For the U.S. MVP, we mark this as a future flag. We can, however, approximate a limited version by looking at patterns in awards: for example, if the same small set of vendors are the only ones ever winning in a certain region or category, it suggests an exclusive arrangement. Or if an agency alternates winners between two companies in successive procurements, that’s unusual. These are heuristics we might explore in analysis but they won’t be formal automated flags in the MVP due to data limits. Notably, prior research using collusion algorithms on procurement data (like checking distribution of bids) has successfully identified cartels\[25]\[26] – indicating the value of this approach once data is available. We will document this as a recommendation for future integration (potentially linking DOJ antitrust case data or state procurement portals that have more bid info).

### 9. Unusually Short or Unfair Tender Timing – **\[Future Flag]**
*   **Global Definition:** Red flags related to the timeline of the procurement. One example is an unusually short advertisement period for a tender – if bids were only invited for, say, 3 days when normally 30 days are allowed, it could indicate the process was rushed to favor a vendor (only the pre-selected company had time to bid). Another example is long delays in award: if there’s a long gap between bid submission deadline and contract award signing, it might signal behind-the-scenes negotiations (possibly of bribes). Both extremely fast or slow timelines, inconsistent with norms, can be warning signs. Additionally, repeated cancellation and re-tendering of a project can be a flag (e.g., if open tenders keep getting canceled until a favored bidder can win via a direct negotiated deal).
*   **USAspending Implementation:** These require procurement process dates that are not in USAspending. USAspending records the award date but not the original tender announcement date or bid deadline. In an OCDS dataset, we would have fields like tender publication date, closing date, award date, and contract start date, which enable calculation of advertisement length and award decision time. With U.S. data, one might extract this from FedBizOpps (now beta.SAM) records, but that’s outside the current dataset. Therefore, we list timing-related flags as conceptual for now. We will note their importance: for instance, a very short bidding window (like a weekend) is a classic red flag often found in procurement audits. If we had the data, we would flag tenders with an open period far below the norm (e.g. 5 days vs a typical 30 days). Similarly, award delay flags could be set if the contract signing happens much later than expected, without official justification – potentially indicating a hold-up as negotiations (possibly corrupt) took place\[27]. In future work, integrating solicitation data from SAM.gov or agency procurement systems could make these flags actionable in the U.S context.

### 10. Others (Future Enhancements)
We recognize there are many more niche indicators documented in literature. For completeness, we will briefly note a few additional future flags:
*   Unreasonably high volume of low-value purchases (could indicate splitting or avoiding oversight),
*   Recurring procurement just below authority limits (similar to threshold splitting, but could be below authorities’ approval limits),
*   Patterns of bid protests or complaints (many protests could signal stakeholders perceive unfairness),
*   Missing key data or transparency (e.g., missing contractor IDs or missing justification documents – the EU scoreboard even flags missing registration numbers as a data integrity issue\[28]).

While our MVP focuses on the most salient and implementable flags, these items remind us that a holistic integrity toolkit can grow over time. Each new data source (e.g., audit reports, complaint databases, etc.) could unlock new automated checks.

---

## Conclusion

By prioritizing a core set of globally recognized risk indicators and mapping them to available U.S. procurement data fields, our plan ensures that the red flags we highlight are both meaningful and computationally achievable. The first tier of indicators (single-bid awards, non-competitive awards, contract splitting, vendor concentration, excessive modifications, etc.) can be directly calculated using USAspending data, allowing us to build an end-to-end demonstration of how open data can reveal corruption risk in public contracting. The second tier of indicators serves as a forward-looking roadmap – bridging into areas that will require additional data integration (such as bidder-level details or ownership information) and collaboration with other databases or agencies. This two-tier approach balances practicality with ambition: we deliver a tangible monitoring tool for the here-and-now, while aligning with the broader international standards and pointing to future enhancements as data openness improves.

Crucially, all flagged results will be presented with the appropriate context and visualization to aid interpretation. Simple charts and dashboards can help convey patterns (for example, highlighting if Agency Y’s single-bid rate is trending upward year-over-year, or if one supplier suddenly dominates Agency Z’s contracts). Embedding such visuals alongside the analysis (as we plan with small charts and flow diagrams) will make the findings accessible to decision-makers. The use of visuals will be carefully controlled – only employed when they add clarity and are backed by the data (for instance, we won’t show a chart or illustration unless we have computed evidence or authoritative guidance behind it). The ultimate aim is that by combining strong data-driven signals with clear global definitions, and illustrating them with intuitive visuals, we create a powerful tool for procurement oversight. It will demonstrate how open data can be leveraged to shine light on red flags and thereby help mitigate corruption risks, all while being transparent about the methodology and limitations. This approach not only addresses immediate needs in the U.S. context but also lays a foundation that can be scaled globally as more governments adopt open contracting data standards and as additional data connectors (for bidders, owners, etc.) come online\[7]\[6].

## References

*   **\[1] \[5] \[22] \[23] \[27]** [A corruption risk indicator for public procurement - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0176268021001166)
*   **\[2] \[12]** [Red Flags in Public Procurement. A guide to using data to detect and mitigate risks - Open Contracting Partnership](https://www.open-contracting.org/resources/red-flags-in-public-procurement-a-guide-to-using-data-to-detect-and-mitigate-risks/)
*   **\[3] \[4] \[18] \[19] \[20] \[25] \[26]** [From Fishing to Catching: Developing Actionable Red Flags in Public Procurement to Prevent and Control Corruption](https://publications.iadb.org/publications/english/document/From-Fishing-to-Catching-Developing-Actionable-Red-Flags-in-Public-Procurement-to-Prevent-and-Control-Corruption.pdf)
*   **\[6] \[7] \[8]** [Cardinal, an open-source library to calculate public procurement red flags - Open Contracting Partnership](https://www.open-contracting.org/2024/06/12/cardinal-an-open-source-library-to-calculate-public-procurement-red-flags/)
*   **\[9] \[10] \[17] \[24]** [QoG Data](https://datafinder.qog.gu.se/dataset/cri)
*   **\[11]** [GAO-25-107469, FEDERAL SPENDING TRANSPARENCY: Actions Needed to Help Ensure Procurement Data Quality](https://www.gao.gov/assets/gao-25-107469.pdf)
*   **\[13] \[14] \[15] \[16] \[28]** [Public Procurement - Performance per Policy Area - The Single Market Scoreboard - European Commission](https://ec.europa.eu/internal_market/scoreboard/_docs/2020/07/performance_per_policy_area/public_procurement_en.pdf)
*   **\[21]** [Fighting Corruption and Promoting Integrity in Public Procurement (EN)](https://www.oecd.org/content/dam/oecd/en/publications/reports/2005/12/fighting-corruption-and-promoting-integrity-in-public-procurement_g1gh5f23/9789264014008-en.pdf)