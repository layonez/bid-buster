## **Hackathon Rules**

To keep things fair and aligned with our goals, all teams must follow these rules:

- **Open Source:** Everything shown in the demo must be fully open source. This includes every component - backend, frontend, models, and any other parts of the project - published under an approved open source license.

- **New Work Only:** All projects must be started from scratch during the hackathon with no previous work.

- **Team Size:** Teams may have up to **2** members.

- **Banned Projects:** Projects will be disqualified if they: violate legal, ethical, or platform policies, use code, data, or assets you do not have the rights to.

## **4. Problem Statements & Example Projects**

**Problem Statement One:** Build a Tool That Should Exist — Create the AI-native app or workflow you wish someone had already built. Eliminate busywork. Make hard things effortless.

**Example Projects:**

- Contract Lifecycle Autopilot - Extracts all obligations from contracts and tracks deadlines with automatic reminders

- Product Changelog Publisher - Turns release notes into customer-facing announcements across multiple channels

- Bug Report Enricher - Automatically adds system logs, user history, and reproduction steps to support tickets

**Problem Statement Two:** Break the Barriers — Expert knowledge, essential tools, AI's benefits — take something powerful that's locked behind expertise, cost, language, or infrastructure and put it in everyone's hands.

**Example Projects:**

- Crop Doctor — Combines image analysis, weather data, and soil reports to diagnose plant diseases and recommend organic treatment protocols.

- Accessibility Auditor — Evaluates websites, documents, and physical spaces against accessibility standards and generates remediation plans.

- Open Source Hardware Guide — Helps makers navigate component selection, PCB design, and manufacturing for hardware projects.

**Problem Statement Three:** Amplify Human Judgment — Build AI that makes researchers, professionals, and decision-makers dramatically more capable — without taking them out of the loop. The best AI doesn't replace human expertise. It sharpens it.

**Example Projects:**

- Brand Safety Monitor - Reviews ad placements and content adjacencies, flagging reputation risks for marketing teams

- Discovery Anomaly Detector - Flags documents in discovery that should exist but are missing based on references in other documents

- Grading Calibration Partner — Highlights scoring inconsistencies in instructor assessments, supporting but not overriding professional judgment on standards.

## **5. Anthropic-Provided Resources**

**Quickstarts**

[Claude Code Quickstart](https://code.claude.com/docs/en/quickstart)

[Claude API Quickstart](https://platform.claude.com/docs/en/get-started)

[Claude Models Overview](https://platform.claude.com/docs/en/about-claude/models/overview)

**Docs**

[Claude Code Docs](https://code.claude.com/docs)

[Claude API Docs](https://platform.claude.com/docs/en/home)

[MCP Docs](https://modelcontextprotocol.io/docs/getting-started/intro)

[Agent Skills Docs](https://agentskills.io/home)

**Blogs**

[Claude Code Best Practices](https://code.claude.com/docs/en/best-practices)

[Building Effective Agents](https://www.anthropic.com/engineering/building-effective-agents)

[Building Agents with the Claude Agent SDK](https://claude.com/blog/building-agents-with-the-claude-agent-sdk)

[Building multi-agent systems: when and how to use them](https://claude.com/blog/building-multi-agent-systems-when-and-how-to-use-them)

[Best practices for prompt engineering](https://claude.com/blog/best-practices-for-prompt-engineering)

[Effective Context Engineering](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)

[Extending Claude’s capabilities with skills and MCP servers](https://claude.com/blog/extending-claude-capabilities-with-skills-mcp-servers)

[Skills explained: How Skills compares to prompts, Projects, MCP, and subagents](https://claude.com/blog/skills-explained)

[Building agents with Skills: Equipping agents for specialized work](https://claude.com/blog/building-agents-with-skills-equipping-agents-for-specialized-work)

[Claude Code power user customization: How to configure hooks](https://claude.com/blog/how-to-configure-hooks)

## Judging

Judging for the Anthropic Virtual Hackathon happens in two stages:

### Stage 1 – Asynchronous Judging

- Date: Feb 16th - Feb 17th

- How it works:

  - Judges review your submitted projects asynchronously via the judging platform.

  - Each team will have uploaded:

    1. A short demo video (3 minute maximum)

    2. Open Source Project repository / code

    3. Written summary (100–200 words)

  - Judges independently evaluate projects using standardized criteria.\
    Judging Criteria:

    1. Impact (25%) - What's the real-world potential here? Who benefits, and how much does it matter? Could this actually become something people use? Does it fit into one of the problem statements listed above?

    2. Opus 4.6 Use (25%) - How creatively did this team use Opus 4.6? Did they go beyond a basic integration? Did they surface capabilities that surprised even us?

    3. Depth & Execution (20%) — Did the team push past their first idea? Is the engineering sound and thoughtfully refined? Does this feel like something that was wrestled with — real craft, not just a quick hack?

    4. Demo (30%) *—* Is this a working, impressive demo? Does it hold up live? Is it genuinely cool to watch?

  - After evaluation, the team aggregates scores to determine the Top 6 projects for the final round

### Stage 2 – Final Round Live Judging

- Date & Time: Feb 18th, 12:00PM EST

- Format:

  - During the live session, pre-recorded demos from each team will be played (3 minutes per team).

  - Judges will deliberate after all demos to determine the 1st, 2nd, and 3rd place along with extra prizes.
